{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACDC Tutorial <a class=\"jp-toc-ignore\"></a>\n",
    "This Jupyter notebook demonstrates how to run **ASCENT** on the **ACDC** dataset. The dataset utilized in this tutorial is a cropped version of the original dataset. You can find the original dataset [here](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/637218c173e9f0047faa00fb).\n",
    "\n",
    "Reference:  \n",
    "*O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, et al. \"Deep Learning Techniques for Automatic MRI Cardiac Multi-structures Segmentation and Diagnosis: Is the Problem Solved ?\" in IEEE Transactions on Medical Imaging, vol. 37, no. 11, pp. 2514-2525, Nov. 2018.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setup and Dependencies Installation <a class=\"anchor\" id=\"setup\"></a>\n",
    "The first step is to install the necessary dependencies. Please follow the instructions in the **Install** section in the [README](../README.md) file to create a conda environment and install the required dependencies. Make sure you have created and activated the `conda` environment as per the README instructions. If you haven't installed the required dependencies yet, execute the following cell to install them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)\n",
    "\n",
    "%%capture packages_install\n",
    "# Install PyTorch\n",
    "%conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y\n",
    "\n",
    "# Install ASCENt as an editable package\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Dataset and Preprocessing <a class=\"anchor\" id=\"dataset_preprocessing\"></a>\n",
    "\n",
    "### A. Download the dataset <a class=\"anchor\" id=\"download\"></a>\n",
    "Once the environment is successfully set up, download the ACDC dataset by executing the following cell. The dataset will be downloaded to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if \"./\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"./\")\n",
    "    print(sys.path)\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"./ACDC\").is_dir():\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    zipurl = \"https://humanheart-project.creatis.insa-lyon.fr/database/api/v1/collection/637218c173e9f0047faa00fb/download\"\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with zipfile.ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            for member in tqdm(\n",
    "                zfile.infolist(), desc=\"Downloading and extracting data\", position=0, leave=True\n",
    "            ):\n",
    "                try:\n",
    "                    zfile.extract(member, \"./\")\n",
    "                except zipfile.error as e:\n",
    "                    pass\n",
    "\n",
    "acdc_data_path = \"./ACDC/database\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Convert the dataset to nnU-Net format\n",
    "\n",
    "Next, we reformat the dataset to the nnU-Net format. You can find the conversion script we use [here](../ascent/dataset_conversion/acdc.py). For the purpose of this tutorial and to save time, we will be working with a cropped version of the ACDC dataset. Execute the following cells to convert the dataset to the nnU-Net format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell will show the help message for the ACDC dataset conversion script\n",
    "%run -i ../ascent/dataset_conversion/acdc.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell will crop the ACDC dataset and convert it to the nnU-Net format\n",
    "# Use -cf flag to crop the dataset\n",
    "%run -i ../ascent/dataset_conversion/acdc.py -d {acdc_data_path} -o \"../data\" -n \"ACDC\" -cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Preprocess the dataset and generate experiment plans\n",
    "\n",
    "The converted dataset is now located in `../data/ACDC/raw`. We will proceed to preprocess the dataset and generate experiment plans by executing the following cell. The preprocessed dataset will be located in `../data/ACDC/preprocessed`. During preprocessing, the data is cropped to non-zero regions, resampled to median image spacing, and normalized using z-score normalization. The preprocessed data are stored in `.npz` format for faster loading after being unpacked to `.npy`.\n",
    "\n",
    "> **ⓘ**\n",
    "> In the next cell, we are using `%run` magic command of jupyter to achieve a more visually appealing and real-time output. In a real terminal environment, simply use the `ascent_preprocess_and_plan` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../ascent/preprocess_and_plan.py dataset=ACDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YAML** plans for training 2D and 3D nnU-Nets are generated and stored in `../ascent/configs`, starting with the `acdc_` prefix. Examine the generated 3D model plan by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML(typ=\"safe\")\n",
    "plan = yaml.load(open(\"../ascent/configs/model/acdc_3d.yaml\"))\n",
    "print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training and Validation\n",
    "We are now prepared to train the model. In this step, a 3D nnU-Net model will be trained on the ACDC dataset using the `acdc_3d` experiment plan. We will use the tensorboard logger to monitor the training process, and other loggers such as wandb and Comet can also be employed.\n",
    "\n",
    "The log folder to save the training logs will be provided at the end of the training. Execute the following cell to start the training. We will use a reduced number of epochs and batch size to save time and resources. You can retain the default values for better performance.\n",
    "\n",
    "> **ⓘ**\n",
    "> By default, the mixed precision is activated to speed up the training process. For Windows users, mixed precision might have issues. You can deactivate it by adding `trainer.precision=32` in the command line arguments in the next cell.  \n",
    "> **ⓘ**\n",
    "> In the next cell, we are using the `%run` magic command in Jupyter for a more visually appealing and real-time output. In a real terminal environment, simply use the `ascent_train` command.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets\n",
    "except ModuleNotFoundError:\n",
    "    %pip install ipywidgets\n",
    "\n",
    "model = \"3d\"  # Change to \"2d\" for 2D model\n",
    "\n",
    "# Add trainer.accelerator=cpu to train on CPU\n",
    "%run -i ../ascent/train.py experiment=acdc_{model} logger=tensorboard datamodule.test_splits=False datamodule.batch_size=2 fold=0 trainer.max_epochs=2 test=False trainer.precision=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training process using TensorBoard by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Copy and paste output dir returned from the previous cell here\n",
    "output_dir = r\"<PASTE_HERE>\"\n",
    "output_dir = Path(output_dir)\n",
    "tensorboard_dir = str((output_dir / \"tensorboard\").as_posix())\n",
    "\n",
    "# Install tensorboard\n",
    "try:\n",
    "    import tensorboard\n",
    "except ModuleNotFoundError:\n",
    "    %pip install tensorboard\n",
    "# Launch tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {tensorboard_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, `ascent_train` trains the model and uses the best weights to run inference on validation data if `test=True` is set. Due to problems with Jupyter, we will manually run the inference on the validation set in the next cell.\n",
    "> **ⓘ**\n",
    "> In the next cell, we are using the `%run` magic command in Jupyter for a more visually appealing and real-time output. In a real terminal environment, simply use the `ascent_evaluate` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path\n",
    "ckpt = str((output_dir / \"checkpoints\" / \"last.ckpt\").as_posix())\n",
    "\n",
    "%run -i ../ascent/eval.py experiment=acdc_{model} logger=tensorboard datamodule.test_splits=False fold=0 trainer.precision=32 ckpt_path={ckpt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Inference\n",
    "Now, we will run inference on new data, such as the test data, using the trained model. The inference results will be saved in the `./inference` folder. Execute the following cell to run inference on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input folder\n",
    "input_folder = \"../data/ACDC/raw/imagesTs\"\n",
    "\n",
    "# Specify the output folder\n",
    "output_folder = \"./inference\"\n",
    "\n",
    "%run -i ../ascent/predict.py dataset=ACDC model=acdc_{model} trainer.precision=32 ckpt_path={ckpt} input_folder={input_folder} output_folder={output_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Visualization\n",
    "Finally, visualize the results of the inference, including the input image, ground truth, and the predicted segmentation mask, by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the GT of the test set\n",
    "gt_folder = \"../data/ACDC/raw/labelsTs\"\n",
    "\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from ascent.utils.file_and_folder_operations import subfiles\n",
    "from ascent.utils.visualization import imagesc\n",
    "\n",
    "pred_dir = Path(output_folder) / \"inference_raw\"\n",
    "\n",
    "# Get a random index to display an image with label from inference folder\n",
    "all_inference_files = subfiles(pred_dir, suffix=\".nii.gz\", join=False)\n",
    "idx = np.random.randint(len(all_inference_files))\n",
    "\n",
    "# Print the selected case\n",
    "print(\"selected case: \", all_inference_files[idx][:-7])\n",
    "\n",
    "# Load the image, label, and prediction\n",
    "image = sitk.ReadImage(str(Path(input_folder) / f\"{all_inference_files[idx][:-7]}_0000.nii.gz\"))\n",
    "label = sitk.ReadImage(str(Path(gt_folder) / all_inference_files[idx]))\n",
    "pred = sitk.ReadImage(str(Path(pred_dir) / all_inference_files[idx]))\n",
    "image_array = sitk.GetArrayFromImage(image)\n",
    "label_array = sitk.GetArrayFromImage(label)\n",
    "pred_array = sitk.GetArrayFromImage(pred)\n",
    "\n",
    "# Select a random slice\n",
    "slice_idx = np.random.randint(image_array.shape[0])\n",
    "print(\"selected slice: \", slice_idx)\n",
    "\n",
    "# Plot the image and label\n",
    "colors = [\"black\", \"red\", \"green\", \"blue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "ax = figure.add_subplot(1, 3, 1)\n",
    "imagesc(ax, image_array[slice_idx], title=\"Image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(1, 3, 2)\n",
    "imagesc(\n",
    "    ax,\n",
    "    label_array[slice_idx],\n",
    "    title=\"Ground Truth\",\n",
    "    show_colorbar=False,\n",
    "    colormap=cmap,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "ax = figure.add_subplot(1, 3, 3)\n",
    "imagesc(\n",
    "    ax,\n",
    "    pred_array[slice_idx],\n",
    "    title=\"Prediction\",\n",
    "    show_colorbar=False,\n",
    "    colormap=cmap,\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ascent]",
   "language": "python",
   "name": "conda-env-.conda-ascent-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
